
network_name: VGG19Simple
dataset_name: break_his
dataset_count: (1463, 309, 309)
classes: ['ADE', 'DUC', 'FIB', 'LOB', 'MUC', 'PAP', 'PHY', 'TUB']
image_size: (150, 150)
data_augmentation: True
batch_size: 64
loss: categorical_crossentropy
learning_rate: 0.0001
optimizer: rmsprop
metrics: ['acc']
epochs: 90
skip_filters: True
train_generator: <keras.preprocessing.image.DirectoryIterator object at 0x7f1db1246c18>
validation_generator: <keras.preprocessing.image.DirectoryIterator object at 0x7f1e800730f0>
test_generator: <keras.preprocessing.image.DirectoryIterator object at 0x7f1eabc7fbe0>
weights: imagenet
include_top: False
convolutional_base: <keras.engine.training.Model object at 0x7f1e802a6780>
model: <keras.engine.training.Model object at 0x7f1db1305358>
history: <keras.callbacks.callbacks.History object at 0x7f1db1722b70>
predictions: [0 0 0 0 0 2 0 0 0 0 0 0 0 0 7 7 3 1 1 1 1 1 1 1 1 1 1 1 1 7 1 1 1 1 3 3 3
 5 1 1 5 1 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 7 1 1 3 1 1 1 1 1 1 4 1 1 3 4 1 1
 1 1 1 5 4 1 1 1 3 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 3 1 1 1 3 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 5 4 1 1 1
 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 7 2 2 1 1 2 2 5 2 2 2
 2 2 2 7 7 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 1 3 3 7 2 4 4 1 4 4
 4 3 4 4 4 4 4 4 4 2 4 4 4 4 4 4 5 4 4 4 4 4 4 0 4 4 5 5 5 7 5 5 5 5 5 5 5
 5 5 3 1 1 5 5 5 5 5 2 6 4 2 6 3 6 6 6 6 6 6 6 6 6 6 0 7 7 2 7 7 7 7 7 7 7
 7 7 7 7 7 7 7 7 7 7 7 7 7]
fine_tune: True
first_trainable_block: 4
fine_tune_learning_rate: 4e-05
fine_tune_epochs: 80


Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv4 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv4 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 8192)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               4194816   
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1024)              525312    
_________________________________________________________________
dropout_2 (Dropout)          (None, 1024)              0         
_________________________________________________________________
predictions (Dense)          (None, 8)                 8200      
=================================================================
Total params: 24,752,712
Trainable params: 22,427,144
Non-trainable params: 2,325,568
_________________________________________________________________


training and validation accuracy and loss: 

    accuracy      loss  validation accuracy  validation loss
0   0.360257  2.451109             0.496094         1.426991
1   0.393466  1.781975             0.412245         1.629853
2   0.444604  1.664857             0.424490         1.566899
3   0.460329  1.598379             0.489796         1.426979
4   0.496783  1.457441             0.497959         1.385174
5   0.524660  1.366283             0.550781         1.279359
6   0.560369  1.290487             0.530612         1.214211
7   0.580576  1.213264             0.497959         1.150987
8   0.569693  1.215524             0.661224         1.004865
9   0.600852  1.122543             0.608163         1.063128
10  0.615827  1.064250             0.687500         0.938217
11  0.626420  1.044399             0.616327         1.266634
12  0.625447  1.016921             0.689796         0.870424
13  0.659042  0.914090             0.718367         0.824230
14  0.659712  0.885756             0.657143         0.747097
15  0.696023  0.856975             0.687500         0.790328
16  0.671908  0.864128             0.648980         1.064294
17  0.715511  0.787202             0.661224         0.844921
18  0.714796  0.793707             0.644898         0.615202
19  0.729807  0.749524             0.730612         0.918948
20  0.731951  0.746827             0.707031         1.197051
21  0.723374  0.768070             0.730612         0.978126
22  0.756969  0.653234             0.395918         1.717228
23  0.756254  0.686121             0.738775         0.654041
24  0.779830  0.583114             0.624490         1.103347
25  0.752518  0.654482             0.636719         0.850182
26  0.775554  0.589944             0.718367         0.794288
27  0.789135  0.567677             0.742857         0.658316
28  0.809149  0.510685             0.767347         0.776490
29  0.812723  0.549877             0.808163         0.705524
30  0.802001  0.613048             0.726562         0.721122
31  0.822443  0.464756             0.636735         1.230145
32  0.806290  0.537919             0.706122         0.959176
33  0.825590  0.476801             0.751020         0.772562
34  0.851322  0.406535             0.755102         1.117861
35  0.837741  0.470864             0.839844         0.596740
36  0.841007  0.462012             0.673469         1.024874
37  0.840909  0.435558             0.673469         0.792769
38  0.863309  0.408522             0.791837         0.471594
39  0.854403  0.397150             0.759184         0.936796
40  0.875625  0.362807             0.722656         0.823525
41  0.864904  0.377523             0.742857         0.941994
42  0.865618  0.404819             0.718367         1.106460
43  0.846319  0.471890             0.653061         1.560454
44  0.877770  0.361718             0.783673         1.518595
45  0.872766  0.340069             0.640625         1.526442
46  0.870622  0.353738             0.677551         1.403004
47  0.868477  0.399489             0.808163         1.031094
48  0.870622  0.373242             0.763265         1.174316
49  0.879914  0.346561             0.734694         0.948373
50  0.889921  0.311239             0.722656         0.988247
51  0.887777  0.319928             0.559184         1.366084
52  0.882812  0.362325             0.755102         0.658407
53  0.904217  0.240929             0.808163         0.616334
54  0.880576  0.377568             0.791837         0.375809
55  0.894925  0.317525             0.765625         0.913262
56  0.897069  0.327949             0.746939         0.880508
57  0.905647  0.270385             0.751020         0.971988
58  0.876420  0.347711             0.755102         0.705620
59  0.916547  0.229680             0.677551         1.325453
60  0.875710  0.378604             0.800781         1.114872
61  0.930935  0.213048             0.759184         1.209629
62  0.899148  0.283170             0.722449         1.700586
63  0.884918  0.390707             0.800000         0.875427
64  0.904217  0.274361             0.857143         0.335543
65  0.899929  0.359482             0.832031         0.673668
66  0.929235  0.229238             0.828571         0.264910
67  0.909936  0.307772             0.759184         0.678136
68  0.922087  0.221276             0.653061         2.476782
69  0.896355  0.349205             0.644898         1.467884
70  0.927091  0.215722             0.769531         0.594425
71  0.905647  0.262656             0.795918         0.628572
72  0.925426  0.220543             0.848980         0.565326
73  0.898561  0.332868             0.808163         0.804105
74  0.907076  0.274295             0.706122         1.427064
75  0.920658  0.250424             0.671875         1.365643
76  0.911365  0.261595             0.718367         1.541884
77  0.913352  0.324793             0.800000         0.562354
78  0.926376  0.198079             0.783673         1.509339
79  0.927338  0.229409             0.853061         0.724021

test accuracy: 0.8359375
test loss: 1.0225623846054077